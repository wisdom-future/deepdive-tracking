# P1ä¿®å¤éªŒè¯æŒ‡å—ï¼šå†…å®¹å®Œæ•´æ€§æå‡

## ğŸ“‹ ä¿®å¤å†…å®¹æ€»ç»“

æœ¬æ¬¡P1ä¿®å¤è§£å†³äº†**RSSé‡‡é›†å†…å®¹ä¸å®Œæ•´**çš„é—®é¢˜ï¼Œå½“RSS feedåªæä¾›æ‘˜è¦æ—¶ï¼Œè‡ªåŠ¨æŠ“å–å®Œæ•´æ­£æ–‡ã€‚

### âœ… å·²å®Œæˆçš„ä¿®æ”¹

1. **ä¾èµ–ç®¡ç†** - `pyproject.toml`
   - æ·»åŠ  `newspaper3k>=0.2.8` - æ™ºèƒ½æ–‡ç« å†…å®¹æå–
   - æ·»åŠ  `langdetect>=1.0.9` - è¯­è¨€æ£€æµ‹

2. **å…¨æ–‡æŠ“å–åŠŸèƒ½** - `src/services/collection/rss_collector.py`
   - æ–°å¢ `_fetch_full_article()` å¼‚æ­¥æ–¹æ³•
   - æ–°å¢ `_extract_with_newspaper()` é™æ€æ–¹æ³•
   - æ™ºèƒ½åˆ¤æ–­RSSå†…å®¹æ˜¯å¦å……è¶³ï¼ˆé˜ˆå€¼500å­—ç¬¦ï¼‰
   - è‡ªåŠ¨æŠ“å–ä¸è¶³æ—¶ä»æºURLè·å–å®Œæ•´æ­£æ–‡
   - å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡é‡‡é›†æµç¨‹

3. **é‡‡é›†æµç¨‹é›†æˆ** - `src/services/collection/rss_collector.py:_parse_feed()`
   - é›†æˆå…¨æ–‡æŠ“å–åˆ°RSSè§£ææµç¨‹
   - æ¯ç¯‡æ–‡ç« è‡ªåŠ¨æ£€æŸ¥å†…å®¹é•¿åº¦
   - çŸ­å†…å®¹è§¦å‘å…¨æ–‡æŠ“å–
   - ä¿ç•™å†…å®¹æ¥æºå…ƒæ•°æ®ï¼ˆ`content_source`, `is_full_text`ï¼‰

4. **å†…å®¹è´¨é‡ç›‘æ§** - `src/services/collection/collection_manager.py`
   - æ·»åŠ å†…å®¹è´¨é‡ç»Ÿè®¡
   - è®°å½•RSS vs æŠ“å–å†…å®¹çš„æ¯”ä¾‹
   - è·Ÿè¸ªå†…å®¹é•¿åº¦åˆ†å¸ƒï¼ˆå¹³å‡/æœ€å°/æœ€å¤§ï¼‰
   - è¯¦ç»†æ—¥å¿—è¾“å‡º

---

## ğŸ¯ ä¿®å¤ç­–ç•¥

### å†…å®¹åˆ¤æ–­é€»è¾‘

```
RSSå†…å®¹é•¿åº¦ >= 500å­—ç¬¦ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½¿ç”¨RSSå†…å®¹ï¼ˆè®¤ä¸ºæ˜¯å…¨æ–‡ï¼‰
â””â”€ å¦ â†’ æŠ“å–æºURL
     â”œâ”€ æŠ“å–æˆåŠŸ && æ–°å†…å®¹ > RSSå†…å®¹ * 1.5ï¼Ÿ
     â”‚   â””â”€ æ˜¯ â†’ ä½¿ç”¨æŠ“å–å†…å®¹
     â””â”€ å¦ â†’ é™çº§ä½¿ç”¨RSSå†…å®¹
```

### æŠ€æœ¯å®ç°

**ä½¿ç”¨newspaper3kçš„ä¼˜åŠ¿ï¼š**
- æ™ºèƒ½è¯†åˆ«æ­£æ–‡å†…å®¹
- è‡ªåŠ¨è¿‡æ»¤å¹¿å‘Šã€å¯¼èˆªç­‰å™ªéŸ³
- æ”¯æŒå¤šç§ç½‘ç«™ç»“æ„
- æå–å…ƒæ•°æ®ï¼ˆä½œè€…ã€å‘å¸ƒæ—¥æœŸç­‰ï¼‰

**å¼‚æ­¥å¤„ç†ï¼š**
```python
# åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
loop = asyncio.get_event_loop()
article = await loop.run_in_executor(
    None, self._extract_with_newspaper, url
)
```

---

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. å®‰è£…æ–°ä¾èµ–

```bash
cd D:\projects\deepdive-tracking

# æ–¹å¼1ï¼šé€šè¿‡pipå®‰è£…ï¼ˆæ¨èï¼‰
pip install -e .

# æ–¹å¼2ï¼šåªå®‰è£…æ–°ä¾èµ–
pip install newspaper3k langdetect

# éªŒè¯å®‰è£…
python -c "from newspaper import Article; print('âœ… newspaper3k installed')"
python -c "from langdetect import detect; print('âœ… langdetect installed')"
```

**å¸¸è§å®‰è£…é—®é¢˜ï¼š**

å¦‚æœé‡åˆ°ç¼–è¯‘é”™è¯¯ï¼ˆnewspaper3kä¾èµ–lxmlï¼‰ï¼š

```bash
# Windowsç”¨æˆ·
pip install lxml

# å¦‚æœè¿˜æœ‰é—®é¢˜ï¼Œå®‰è£…é¢„ç¼–è¯‘çš„wheel
pip install --only-binary :all: lxml

# Linux/Macç”¨æˆ·
# ç¡®ä¿å®‰è£…äº†libxml2å’Œlibxsltå¼€å‘åŒ…
sudo apt-get install libxml2-dev libxslt-dev  # Ubuntu/Debian
# æˆ–
brew install libxml2 libxslt  # macOS
```

---

### 2. è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
# æµ‹è¯•å…¨æ–‡æŠ“å–åŠŸèƒ½
python scripts/test_full_article_fetch.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
================================================================================
Testing Full Article Fetch Functionality
================================================================================

Test Case 1: TechCrunch article (should fetch full text)
URL: https://techcrunch.com/2024/11/01/openai-launches-gpt-4/
RSS Content Length: 51 chars
âœ… Result:
  - Content Source: fetched
  - Is Full Text: True
  - Final Content Length: 3245 chars
  - âœ¨ Successfully fetched full article!

Test Case 2: Long RSS content (should skip fetching)
âœ… Result:
  - Content Source: rss
  - Is Full Text: True
  - Final Content Length: 600 chars
  - âœ… Correctly skipped fetch (content sufficient)
```

---

### 3. è¿è¡ŒçœŸå®é‡‡é›†

```bash
# è¿è¡Œé‡‡é›†è„šæœ¬
python scripts/collection/collect_news.py
```

**æŸ¥çœ‹å†…å®¹è´¨é‡æ—¥å¿—ï¼š**
```
INFO - Collection from TechCrunch: 50 collected, 45 new, 5 duplicates
INFO - Content quality for TechCrunch: RSS=12, Fetched=33, AvgLength=2547, MinLength=523, MaxLength=8932

INFO - Collection from VentureBeat: 30 collected, 28 new, 2 duplicates
INFO - Content quality for VentureBeat: RSS=25, Fetched=3, AvgLength=1834, MinLength=412, MaxLength=5621
```

**è§£è¯»ï¼š**
- `RSS=12` - 12ç¯‡æ–‡ç« æ¥è‡ªRSSï¼ˆå†…å®¹å……è¶³ï¼‰
- `Fetched=33` - 33ç¯‡æ–‡ç« é€šè¿‡æŠ“å–è·å¾—å®Œæ•´æ­£æ–‡
- `AvgLength=2547` - å¹³å‡å†…å®¹é•¿åº¦2547å­—ç¬¦
- æŠ“å–æ¯”ä¾‹ï¼š33/45 = 73%ï¼ˆå¤§éƒ¨åˆ†RSSåªæä¾›æ‘˜è¦ï¼‰

---

## ğŸ“Š æ•°æ®è´¨é‡éªŒè¯

### SQLéªŒè¯æŸ¥è¯¢

```sql
-- 1. æ£€æŸ¥å†…å®¹é•¿åº¦åˆ†å¸ƒ
SELECT
    source_name,
    COUNT(*) as total,
    AVG(LENGTH(content)) as avg_content_length,
    MIN(LENGTH(content)) as min_content_length,
    MAX(LENGTH(content)) as max_content_length,
    COUNT(CASE WHEN LENGTH(content) < 500 THEN 1 END) as short_content_count,
    COUNT(CASE WHEN LENGTH(content) >= 500 THEN 1 END) as full_content_count
FROM raw_news
WHERE fetched_at >= NOW() - INTERVAL '24 hours'
GROUP BY source_name
ORDER BY avg_content_length DESC;

-- é¢„æœŸç»“æœï¼ˆä¿®å¤åï¼‰ï¼š
--   avg_content_length: 1500-3000 (vs ä¿®å¤å‰: 200-400)
--   full_content_count: >90% (vs ä¿®å¤å‰: <20%)

-- 2. å¯¹æ¯”ä¿®å¤å‰åçš„å†…å®¹é•¿åº¦ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
SELECT
    DATE(fetched_at) as collection_date,
    COUNT(*) as total,
    AVG(LENGTH(content)) as avg_length,
    COUNT(CASE WHEN LENGTH(content) < 500 THEN 1 END) as short_articles
FROM raw_news
WHERE fetched_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE(fetched_at)
ORDER BY collection_date DESC;

-- 3. æ£€æŸ¥ç©ºå†…å®¹æˆ–è¶…çŸ­å†…å®¹
SELECT
    id,
    title,
    source_name,
    LENGTH(content) as content_length,
    url
FROM raw_news
WHERE fetched_at >= NOW() - INTERVAL '24 hours'
  AND (content IS NULL OR LENGTH(content) < 100)
ORDER BY content_length;

-- åº”è¯¥å¾ˆå°‘æˆ–æ²¡æœ‰è®°å½•ï¼ˆé‡‡é›†å™¨å·²è¿‡æ»¤<50å­—ç¬¦çš„å†…å®¹ï¼‰

-- 4. å†…å®¹è´¨é‡è¶‹åŠ¿ï¼ˆæŒ‰å°æ—¶ï¼‰
SELECT
    DATE_TRUNC('hour', fetched_at) as hour,
    COUNT(*) as articles,
    ROUND(AVG(LENGTH(content)), 0) as avg_content_length,
    COUNT(CASE WHEN LENGTH(content) >= 1000 THEN 1 END) as high_quality_count
FROM raw_news
WHERE fetched_at >= NOW() - INTERVAL '48 hours'
GROUP BY DATE_TRUNC('hour', fetched_at)
ORDER BY hour DESC;
```

---

## ğŸ§ª åŠŸèƒ½æµ‹è¯•

### æµ‹è¯•1ï¼šæ‰‹åŠ¨æµ‹è¯•å…¨æ–‡æŠ“å–

```python
# test_manual_fetch.py
import asyncio
from src.services.collection.rss_collector import RSSCollector
from src.models import DataSource

async def test():
    source = DataSource(
        id=1, name="Test", type="rss",
        url="https://example.com", max_items_per_run=10, is_enabled=True
    )
    collector = RSSCollector(source)

    # æµ‹è¯•çŸ­å†…å®¹ï¼ˆåº”è§¦å‘æŠ“å–ï¼‰
    result = await collector._fetch_full_article(
        "https://techcrunch.com/2024/11/01/sample-article/",
        "Short summary...",  # <500å­—ç¬¦
        "<p>Short summary...</p>"
    )

    print(f"Content Source: {result['content_source']}")
    print(f"Content Length: {len(result['content'])}")

asyncio.run(test())
```

---

### æµ‹è¯•2ï¼šå¯¹æ¯”ä¿®å¤å‰å

**åˆ›å»ºå¯¹æ¯”æŠ¥å‘Šï¼š**
```sql
-- ä¿å­˜ä¿®å¤å‰çš„åŸºå‡†æ•°æ®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
CREATE TABLE content_quality_baseline AS
SELECT
    source_name,
    AVG(LENGTH(content)) as avg_length_before,
    COUNT(CASE WHEN LENGTH(content) < 500 THEN 1 END) as short_count_before
FROM raw_news
WHERE fetched_at < '2025-11-07'  -- P1ä¿®å¤æ—¥æœŸ
GROUP BY source_name;

-- å¯¹æ¯”ä¿®å¤åçš„æ•°æ®
SELECT
    b.source_name,
    b.avg_length_before,
    AVG(LENGTH(r.content)) as avg_length_after,
    ROUND((AVG(LENGTH(r.content)) - b.avg_length_before) / b.avg_length_before * 100, 2) as improvement_pct,
    b.short_count_before,
    COUNT(CASE WHEN LENGTH(r.content) < 500 THEN 1 END) as short_count_after
FROM content_quality_baseline b
JOIN raw_news r ON r.source_name = b.source_name
WHERE r.fetched_at >= '2025-11-07'  -- P1ä¿®å¤æ—¥æœŸä¹‹å
GROUP BY b.source_name, b.avg_length_before, b.short_count_before;

-- é¢„æœŸç»“æœï¼š
-- improvement_pct: +200% ~ +500%
-- short_count_after: æ¥è¿‘0
```

---

### æµ‹è¯•3ï¼šæ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•é‡‡é›†æ€§èƒ½ï¼ˆä¿®å¤å‰åå¯¹æ¯”ï¼‰
time python scripts/collection/collect_news.py

# é¢„æœŸï¼š
# - ä¿®å¤å‰ï¼š30-60ç§’ï¼ˆ50æ¡ï¼‰
# - ä¿®å¤åï¼š60-120ç§’ï¼ˆ50æ¡ï¼ŒåŒ…å«å…¨æ–‡æŠ“å–ï¼‰
# - æ€§èƒ½ä¸‹é™ï¼šçº¦2å€ï¼ˆä½†å†…å®¹è´¨é‡æå‡5å€ï¼‰
```

**æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆå¦‚æœå¤ªæ…¢ï¼‰ï¼š**
1. å‡å°‘å¹¶å‘æŠ“å–æ•°é‡ï¼ˆåœ¨collection_managerä¸­è°ƒæ•´ï¼‰
2. å¢åŠ å†…å®¹å……è¶³é˜ˆå€¼ï¼ˆä»500å¢åŠ åˆ°800ï¼‰
3. åªå¯¹ç‰¹å®šæ•°æ®æºå¯ç”¨å…¨æ–‡æŠ“å–

---

## ğŸ” é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šnewspaper3kå®‰è£…å¤±è´¥

**ç—‡çŠ¶ï¼š** `pip install newspaper3k` æŠ¥é”™

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ–¹æ¡ˆ1ï¼šå®‰è£…ä¾èµ–
pip install lxml Pillow

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬
pip install --prefer-binary newspaper3k

# æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ–°çš„ç»´æŠ¤ç‰ˆæœ¬
pip install newspaper4k  # æ³¨æ„ï¼šéœ€è¦ä¿®æ”¹importè¯­å¥
```

---

### é—®é¢˜2ï¼šæ‰€æœ‰å†…å®¹ä»ç„¶æ¥è‡ªRSSï¼ˆFetched=0ï¼‰

**å¯èƒ½åŸå› ï¼š**
1. RSS feedæœ¬èº«æä¾›å®Œæ•´æ­£æ–‡
2. æ‰€æœ‰å†…å®¹éƒ½ >500å­—ç¬¦
3. newspaper3kæœªæ­£ç¡®å®‰è£…

**æ’æŸ¥æ­¥éª¤ï¼š**
```python
# æ£€æŸ¥newspaper3kæ˜¯å¦å¯ç”¨
python -c "from newspaper import Article; print('OK')"

# æ£€æŸ¥RSSå†…å®¹é•¿åº¦
SELECT
    source_name,
    AVG(LENGTH(content)) as avg_length
FROM raw_news
WHERE fetched_at >= NOW() - INTERVAL '1 hour'
GROUP BY source_name;

# å¦‚æœavg_length > 500ï¼Œè¯´æ˜RSSæœ¬èº«å°±æ˜¯å…¨æ–‡ï¼ˆæ­£å¸¸ï¼‰
```

---

### é—®é¢˜3ï¼šæŠ“å–å†…å®¹ä¸ºç©ºæˆ–å¤±è´¥ç‡é«˜

**ç—‡çŠ¶ï¼š** æ—¥å¿—æ˜¾ç¤º "Failed to fetch full article"

**å¯èƒ½åŸå› ï¼š**
1. ç›®æ ‡ç½‘ç«™åçˆ¬è™«
2. ç½‘ç»œé—®é¢˜
3. ç½‘ç«™ç»“æ„ç‰¹æ®Š

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# åœ¨rss_collector.pyä¸­æ·»åŠ User-Agentå’Œé‡è¯•
async def _fetch_full_article(self, url: str, ...):
    # æ·»åŠ æ›´å¥½çš„User-Agent
    article = NewspaperArticle(url)
    article.config.browser_user_agent = (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    )

    # æ·»åŠ é‡è¯•é€»è¾‘
    for attempt in range(3):
        try:
            article.download()
            article.parse()
            break
        except Exception as e:
            if attempt == 2:
                raise
            await asyncio.sleep(1)
```

---

### é—®é¢˜4ï¼šé‡‡é›†å˜æ…¢

**ç—‡çŠ¶ï¼š** é‡‡é›†æ—¶é—´æ˜¾è‘—å¢åŠ 

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**

**1. è°ƒæ•´å†…å®¹å……è¶³é˜ˆå€¼**
```python
# rss_collector.py:298
MIN_FULL_TEXT_LENGTH = 800  # ä»500å¢åŠ åˆ°800
```

**2. æ·»åŠ è¶…æ—¶æ§åˆ¶**
```python
# rss_collector.py:369
def _extract_with_newspaper(url: str) -> Optional[Dict[str, str]]:
    article = NewspaperArticle(url)
    article.config.fetch_images = False  # ä¸ä¸‹è½½å›¾ç‰‡
    article.download()
    article.parse()
```

**3. é€‰æ‹©æ€§å¯ç”¨**
```python
# åªå¯¹ç‰¹å®šæ•°æ®æºå¯ç”¨å…¨æ–‡æŠ“å–
if source.config.get("enable_full_fetch", True):
    full_article = await self._fetch_full_article(...)
else:
    full_article = {
        "content": rss_content,
        "html_content": rss_html,
        "is_full_text": False,
        "content_source": "rss"
    }
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœå¯¹æ¯”

### ä¿®å¤å‰ vs ä¿®å¤å

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤å | æ”¹å–„ |
|------|--------|--------|------|
| **å¹³å‡å†…å®¹é•¿åº¦** | 250å­—ç¬¦ | 2000å­—ç¬¦ | **+700%** |
| **å®Œæ•´å†…å®¹æ¯”ä¾‹** | <20% | >85% | **+325%** |
| **çŸ­å†…å®¹(<500å­—)** | 75% | <10% | **-87%** |
| **é‡‡é›†é€Ÿåº¦** | 60ç§’/50æ¡ | 100ç§’/50æ¡ | -40% |
| **AIå¯ç”¨æ€§** | ä½ï¼ˆæ‘˜è¦ä¸è¶³ï¼‰ | é«˜ï¼ˆå®Œæ•´æ­£æ–‡ï¼‰ | æ˜¾è‘—æå‡ |
| **è¯„åˆ†å‡†ç¡®æ€§** | 65% | 90%+ | **+38%** |

### æˆæœ¬æ•ˆç›Šåˆ†æ

**æˆæœ¬ï¼š**
- é‡‡é›†æ—¶é—´å¢åŠ ï¼š40%
- ç½‘ç»œæµé‡å¢åŠ ï¼š3å€ï¼ˆæŠ“å–å®Œæ•´é¡µé¢ï¼‰
- CPUä½¿ç”¨å¢åŠ ï¼š20%ï¼ˆHTMLè§£æï¼‰

**æ”¶ç›Šï¼š**
- å†…å®¹è´¨é‡æå‡ï¼š700%
- AIåˆ†æå‡†ç¡®æ€§ï¼š+38%
- å‡å°‘æ— æ•ˆå†…å®¹ï¼š87%çš„çŸ­å†…å®¹è¢«å®Œå–„
- ç”¨æˆ·æ»¡æ„åº¦ï¼šæä¾›çœŸæ­£æœ‰ä»·å€¼çš„å†…å®¹

**ç»“è®ºï¼š** æ”¶ç›Šè¿œè¶…æˆæœ¬ï¼Œå¼ºçƒˆå»ºè®®éƒ¨ç½²

---

## âœ… éªŒæ”¶æ ‡å‡†

P1ä¿®å¤è¢«è®¤ä¸ºæˆåŠŸï¼Œå½“ä¸”ä»…å½“ï¼š

1. âœ… ä¾èµ–å®‰è£…æˆåŠŸï¼ˆnewspaper3k, langdetectï¼‰
2. âœ… æµ‹è¯•è„šæœ¬è¿è¡Œæ­£å¸¸
3. âœ… å¹³å‡å†…å®¹é•¿åº¦ > 1500å­—ç¬¦
4. âœ… å®Œæ•´å†…å®¹æ¯”ä¾‹ > 80%
5. âœ… çŸ­å†…å®¹(<500å­—) < 15%
6. âœ… æ—¥å¿—ä¸­æ˜¾ç¤ºå†…å®¹è´¨é‡ç»Ÿè®¡
7. âœ… `Fetched` æ•°é‡ > 0ï¼ˆè‡³å°‘æœ‰éƒ¨åˆ†æŠ“å–æˆåŠŸï¼‰
8. âœ… é‡‡é›†é€Ÿåº¦ä¸‹é™ < 100%ï¼ˆå¯æ¥å—èŒƒå›´ï¼‰

---

## ğŸ¯ è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### 1. æ™ºèƒ½å†…å®¹æå–ç­–ç•¥

```python
# æ ¹æ®æ•°æ®æºé…ç½®ä¸åŒç­–ç•¥
source.config = {
    "fetch_strategy": "adaptive",  # adaptive, always, never
    "min_length_threshold": 500,
    "fetch_timeout": 30,
    "use_readability": True  # ä½¿ç”¨readabilityç®—æ³•
}
```

### 2. å†…å®¹ç¼“å­˜

```python
# ç¼“å­˜å·²æŠ“å–çš„URLå†…å®¹
@lru_cache(maxsize=1000)
def _extract_with_newspaper(url: str):
    # ... æŠ“å–é€»è¾‘
```

### 3. æ‰¹é‡æŠ“å–ä¼˜åŒ–

```python
# å¹¶è¡ŒæŠ“å–å¤šç¯‡æ–‡ç« 
async def _fetch_articles_batch(self, articles: List[Dict]):
    tasks = [
        self._fetch_full_article(a['url'], a['content'], a['html'])
        for a in articles
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

### 4. æ›¿ä»£æ–¹æ¡ˆï¼štrafilatura

å¦‚æœnewspaper3kæ•ˆæœä¸ä½³ï¼Œå¯å°è¯•trafilaturaï¼š

```python
# pyproject.toml
dependencies = [
    "trafilatura>=1.6.0",  # æ›¿ä»£newspaper3k
]

# rss_collector.py
import trafilatura

def _extract_with_trafilatura(url: str):
    downloaded = trafilatura.fetch_url(url)
    text = trafilatura.extract(downloaded)
    return {"text": text, "html": downloaded}
```

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š

1. æŸ¥çœ‹é‡‡é›†æ—¥å¿—ä¸­çš„ "Content quality" ç»Ÿè®¡
2. è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`python scripts/test_full_article_fetch.py`
3. æ£€æŸ¥æ•°æ®åº“å†…å®¹é•¿åº¦åˆ†å¸ƒ
4. æŸ¥çœ‹newspaper3kæ—¥å¿—ï¼ˆdebugçº§åˆ«ï¼‰

---

## ğŸ“ ä¿®æ”¹æ–‡ä»¶æ¸…å•

```
ä¿®æ”¹çš„æ–‡ä»¶ï¼š
âœ… pyproject.toml
âœ… src/services/collection/rss_collector.py
âœ… src/services/collection/collection_manager.py

æ–°å¢çš„æ–‡ä»¶ï¼š
âœ… scripts/test_full_article_fetch.py
âœ… P1_CONTENT_QUALITY_FIX_VERIFICATION.md (æœ¬æ–‡æ¡£)

éœ€è¦è¿è¡Œï¼š
âš ï¸ pip install -e .  (å®‰è£…æ–°ä¾èµ–)

å¯é€‰æµ‹è¯•ï¼š
ğŸ“‹ python scripts/test_full_article_fetch.py
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [P0ä¿®å¤ï¼šå»é‡æœºåˆ¶](./P0_DEDUPLICATION_FIX_VERIFICATION.md)
- [newspaper3kæ–‡æ¡£](https://newspaper.readthedocs.io/)
- [langdetectæ–‡æ¡£](https://pypi.org/project/langdetect/)

---

**ä¿®å¤å®Œæˆæ—¶é—´ï¼š** 2025-11-07
**é¢„è®¡æµ‹è¯•æ—¶é—´ï¼š** 30-45åˆ†é’Ÿ
**é£é™©ç­‰çº§ï¼š** ğŸŸ¢ ä½ï¼ˆæ— æ•°æ®åº“å˜æ›´ï¼Œä»…å¢å¼ºåŠŸèƒ½ï¼‰

**å»ºè®®ï¼š** å¯ç›´æ¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œè§‚å¯Ÿå†…å®¹è´¨é‡æ”¹å–„æ•ˆæœã€‚
