# Crawler Collector Configuration Examples

æœ¬æ–‡æ¡£æä¾›CrawlerCollectorçš„é…ç½®ç¤ºä¾‹ï¼Œç”¨äºä»å„ç±»ç½‘ç«™é‡‡é›†å†…å®¹ã€‚

## ğŸ“‹ é…ç½®è¯´æ˜

CrawlerCollectorä½¿ç”¨CSSé€‰æ‹©å™¨ä»ç½‘é¡µæå–å†…å®¹ï¼Œé…ç½®å­˜å‚¨åœ¨`data_sources`è¡¨çš„`config`å­—æ®µï¼ˆJSONBç±»å‹ï¼‰ã€‚

### æ ¸å¿ƒé…ç½®é¡¹

```json
{
  "list_url": "https://example.com/news",
  "list_selector": ".news-item",
  "title_selector": ".title",
  "url_selector": "a[href]",
  "date_selector": ".date",
  "content_selector": ".article-content",
  "author_selector": ".author",
  "fetch_detail": true,
  "use_newspaper": true,
  "pagination": {
    "enabled": true,
    "type": "url_param",
    "param_name": "page",
    "start": 1,
    "max_pages": 5
  }
}
```

---

## ğŸ¯ é…ç½®ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•åˆ—è¡¨é¡µï¼ˆæ— åˆ†é¡µï¼‰

é€‚ç”¨äºï¼šæ–°é—»ç«™é¦–é¡µã€åšå®¢æœ€æ–°æ–‡ç« 

```json
{
  "list_url": "https://techblog.example.com/latest",
  "list_selector": "article.post",
  "title_selector": "h2.post-title",
  "url_selector": "a.post-link",
  "date_selector": "time",
  "author_selector": ".author-name",
  "fetch_detail": true,
  "use_newspaper": true
}
```

**SQLæ’å…¥ç¤ºä¾‹ï¼š**
```sql
INSERT INTO data_sources (name, type, url, config, is_enabled, priority)
VALUES (
  'Tech Blog',
  'crawler',
  'https://techblog.example.com',
  '{
    "list_url": "https://techblog.example.com/latest",
    "list_selector": "article.post",
    "title_selector": "h2.post-title",
    "url_selector": "a.post-link",
    "date_selector": "time",
    "author_selector": ".author-name",
    "fetch_detail": true,
    "use_newspaper": true
  }'::jsonb,
  true,
  50
);
```

---

### ç¤ºä¾‹2ï¼šURLå‚æ•°åˆ†é¡µ

é€‚ç”¨äºï¼šå¤§å¤šæ•°æ–°é—»ç«™ã€è®ºå›

```json
{
  "list_url": "https://news.example.com/ai",
  "list_selector": ".news-list .item",
  "title_selector": "h3.title",
  "url_selector": "a.link[href]",
  "date_selector": "span.publish-time",
  "content_selector": ".article-body",
  "pagination": {
    "enabled": true,
    "type": "url_param",
    "param_name": "page",
    "start": 1,
    "max_pages": 3
  },
  "fetch_detail": true,
  "use_newspaper": false
}
```

**è¯´æ˜ï¼š**
- `type: "url_param"` - é€šè¿‡URLå‚æ•°åˆ†é¡µï¼ˆå¦‚ ?page=1, ?page=2ï¼‰
- `param_name: "page"` - å‚æ•°åç§°
- `start: 1` - èµ·å§‹é¡µç 
- `max_pages: 3` - æœ€å¤šçˆ¬å–3é¡µ

**ç”Ÿæˆçš„URLsï¼š**
```
https://news.example.com/ai?page=1
https://news.example.com/ai?page=2
https://news.example.com/ai?page=3
```

---

### ç¤ºä¾‹3ï¼š"ä¸‹ä¸€é¡µ"é“¾æ¥åˆ†é¡µ

é€‚ç”¨äºï¼šä¼ ç»Ÿè®ºå›ã€æŸäº›æ–°é—»ç«™

```json
{
  "list_url": "https://forum.example.com/ai-news",
  "list_selector": ".topic-row",
  "title_selector": ".topic-title a",
  "url_selector": ".topic-title a",
  "date_selector": ".topic-date",
  "author_selector": ".topic-author",
  "pagination": {
    "enabled": true,
    "type": "next_link",
    "next_selector": ".pagination .next-page",
    "max_pages": 5
  },
  "fetch_detail": true
}
```

**è¯´æ˜ï¼š**
- `type: "next_link"` - é€šè¿‡"ä¸‹ä¸€é¡µ"é“¾æ¥åˆ†é¡µ
- `next_selector` - "ä¸‹ä¸€é¡µ"é“¾æ¥çš„CSSé€‰æ‹©å™¨
- è‡ªåŠ¨è·Ÿéšé“¾æ¥ç›´åˆ°æ²¡æœ‰"ä¸‹ä¸€é¡µ"æˆ–è¾¾åˆ°max_pages

---

### ç¤ºä¾‹4ï¼šåªçˆ¬åˆ—è¡¨é¡µï¼ˆä¸æŠ“å–è¯¦æƒ…ï¼‰

é€‚ç”¨äºï¼šåˆ—è¡¨é¡µå·²æœ‰å®Œæ•´æ‘˜è¦

```json
{
  "list_url": "https://brief.example.com/ai-news",
  "list_selector": ".news-card",
  "title_selector": ".card-title",
  "url_selector": "a.read-more",
  "date_selector": ".card-date",
  "content_selector": ".card-summary",
  "fetch_detail": false,
  "use_newspaper": false
}
```

**è¯´æ˜ï¼š**
- `fetch_detail: false` - ä¸æŠ“å–è¯¦æƒ…é¡µ
- `content_selector` - ç›´æ¥ä»åˆ—è¡¨é¡¹æå–æ‘˜è¦
- é€‚ç”¨äºåˆ—è¡¨é¡µå·²åŒ…å«è¶³å¤Ÿå†…å®¹çš„æƒ…å†µ

---

### ç¤ºä¾‹5ï¼šä½¿ç”¨newspaper3kæ™ºèƒ½æå–

é€‚ç”¨äºï¼šå¤æ‚é¡µé¢ç»“æ„ã€éš¾ä»¥å®šä½CSSé€‰æ‹©å™¨

```json
{
  "list_url": "https://complex-site.example.com/articles",
  "list_selector": ".article-item",
  "title_selector": "h2",
  "url_selector": "a",
  "date_selector": "time",
  "fetch_detail": true,
  "use_newspaper": true
}
```

**è¯´æ˜ï¼š**
- `use_newspaper: true` - ä½¿ç”¨newspaper3kè‡ªåŠ¨æå–æ­£æ–‡
- æ— éœ€é…ç½® `content_selector`
- è‡ªåŠ¨è¯†åˆ«æ­£æ–‡ã€è¿‡æ»¤å¹¿å‘Šå’Œå¯¼èˆª

---

### ç¤ºä¾‹6ï¼šå¤æ‚é€‰æ‹©å™¨

é€‚ç”¨äºï¼šå¤æ‚HTMLç»“æ„

```json
{
  "list_url": "https://complex.example.com/news",
  "list_selector": "div.container > div.row > div.col-md-8 > article",
  "title_selector": "header > h1.entry-title",
  "url_selector": "header > h1.entry-title > a[href]",
  "date_selector": "div.entry-meta time[datetime]",
  "author_selector": "span.author.vcard a",
  "content_selector": "div.entry-content",
  "pagination": {
    "enabled": true,
    "type": "url_param",
    "param_name": "paged",
    "start": 1,
    "max_pages": 2
  }
}
```

---

## ğŸ”§ é€‰æ‹©å™¨è°ƒè¯•æŠ€å·§

### 1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·

```javascript
// åœ¨æµè§ˆå™¨æ§åˆ¶å°æµ‹è¯•é€‰æ‹©å™¨
document.querySelectorAll('.news-item').length
document.querySelector('.news-item .title').textContent
```

### 2. éªŒè¯é€‰æ‹©å™¨

```python
# test_selectors.py
from bs4 import BeautifulSoup
import requests

url = "https://example.com/news"
html = requests.get(url).text
soup = BeautifulSoup(html, 'html.parser')

# æµ‹è¯•åˆ—è¡¨é€‰æ‹©å™¨
items = soup.select('.news-item')
print(f"Found {len(items)} items")

# æµ‹è¯•æ ‡é¢˜é€‰æ‹©å™¨
for item in items[:3]:
    title = item.select_one('.title')
    print(f"Title: {title.get_text() if title else 'NOT FOUND'}")
```

### 3. å¸¸ç”¨é€‰æ‹©å™¨æ¨¡å¼

```css
/* ç±»é€‰æ‹©å™¨ */
.article-item

/* IDé€‰æ‹©å™¨ */
#main-content

/* æ ‡ç­¾é€‰æ‹©å™¨ */
article

/* å±æ€§é€‰æ‹©å™¨ */
a[href]
time[datetime]

/* å­é€‰æ‹©å™¨ */
div.container > article

/* åä»£é€‰æ‹©å™¨ */
div.post h2.title

/* ä¼ªç±»é€‰æ‹©å™¨ */
li:first-child
a:not(.external)

/* ç»„åˆé€‰æ‹©å™¨ */
h1.title, h2.title, h3.title
```

---

## ğŸ“Š å®Œæ•´é…ç½®ç¤ºä¾‹ï¼ˆç”Ÿäº§çº§ï¼‰

```json
{
  "list_url": "https://ai-news.example.com/latest",
  "list_selector": "article.news-card",
  "title_selector": "h2.card-title",
  "url_selector": "a.card-link[href]",
  "date_selector": "time.publish-date[datetime]",
  "author_selector": "span.author-name",
  "content_selector": "div.article-body",
  "fetch_detail": true,
  "use_newspaper": true,
  "pagination": {
    "enabled": true,
    "type": "url_param",
    "param_name": "page",
    "start": 1,
    "max_pages": 5
  }
}
```

**æ•°æ®æºå®Œæ•´è®°å½•ç¤ºä¾‹ï¼š**
```sql
INSERT INTO data_sources (
    name,
    type,
    url,
    priority,
    is_enabled,
    max_items_per_run,
    config,
    default_author,
    tags,
    description
) VALUES (
    'AI News Hub',
    'crawler',
    'https://ai-news.example.com',
    70,
    true,
    50,
    '{
      "list_url": "https://ai-news.example.com/latest",
      "list_selector": "article.news-card",
      "title_selector": "h2.card-title",
      "url_selector": "a.card-link[href]",
      "date_selector": "time.publish-date[datetime]",
      "author_selector": "span.author-name",
      "content_selector": "div.article-body",
      "fetch_detail": true,
      "use_newspaper": true,
      "pagination": {
        "enabled": true,
        "type": "url_param",
        "param_name": "page",
        "start": 1,
        "max_pages": 5
      }
    }'::jsonb,
    'AI News Hub',
    ARRAY['ai', 'machine-learning', 'technology'],
    'Leading AI news aggregator with comprehensive coverage'
);
```

---

## ğŸ§ª æµ‹è¯•é…ç½®

### å¿«é€Ÿæµ‹è¯•è„šæœ¬

```python
# scripts/test_crawler_config.py
import asyncio
from src.database import SessionLocal
from src.models import DataSource
from src.services.collection.crawler_collector import CrawlerCollector

async def test():
    # åˆ›å»ºæµ‹è¯•æ•°æ®æº
    source = DataSource(
        id=999,
        name="Test Crawler",
        type="crawler",
        url="https://example.com",
        config={
            "list_url": "https://example.com/news",
            "list_selector": ".news-item",
            "title_selector": ".title",
            "url_selector": "a[href]",
            "fetch_detail": False
        },
        is_enabled=True
    )

    collector = CrawlerCollector(source)
    articles = await collector.collect()

    print(f"Collected {len(articles)} articles")
    for article in articles[:3]:
        print(f"- {article['title']}")
        print(f"  URL: {article['url']}")

asyncio.run(test())
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. åçˆ¬è™«ç­–ç•¥

å¤§å¤šæ•°ç½‘ç«™æœ‰åçˆ¬è™«æªæ–½ï¼š
- ä½¿ç”¨åˆç†çš„User-Agentï¼ˆå·²å†…ç½®ï¼‰
- æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼ˆå·²å†…ç½®1ç§’å»¶è¿Ÿï¼‰
- é¿å…è¿‡åº¦çˆ¬å–ï¼ˆæ§åˆ¶max_pageså’Œmax_items_per_runï¼‰

### 2. é€‰æ‹©å™¨ç¨³å®šæ€§

- ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åŒ–classï¼ˆå¦‚ `.article`, `.post-title`ï¼‰
- é¿å…ä½¿ç”¨ç”Ÿæˆçš„classï¼ˆå¦‚ `.css-1xa2k3j`ï¼‰
- å®šæœŸæ£€æŸ¥é…ç½®æ˜¯å¦å¤±æ•ˆ

### 3. æ€§èƒ½è€ƒè™‘

```json
{
  "pagination": {
    "max_pages": 3  // ä¸è¦è®¾ç½®å¤ªå¤§ï¼Œé¿å…é‡‡é›†æ—¶é—´è¿‡é•¿
  }
}
```

### 4. å†…å®¹è´¨é‡

- ä¼˜å…ˆä½¿ç”¨ `use_newspaper: true` è·å–é«˜è´¨é‡å†…å®¹
- å¦‚æœnewspaper3kå¤±è´¥ï¼Œä¼šè‡ªåŠ¨é™çº§åˆ°CSSé€‰æ‹©å™¨
- ç¡®ä¿ `content_selector` ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ

---

## ğŸ“š å‚è€ƒèµ„æº

- [CSSé€‰æ‹©å™¨æ•™ç¨‹](https://www.w3schools.com/cssref/css_selectors.asp)
- [BeautifulSoupæ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [newspaper3kæ–‡æ¡£](https://newspaper.readthedocs.io/)

---

**æ›´æ–°æ—¥æœŸï¼š** 2025-11-07
**ç‰ˆæœ¬ï¼š** 1.0
